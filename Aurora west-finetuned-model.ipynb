{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from aurora import Batch, Metadata, AuroraSmallPretrained\n",
    "from datetime import datetime, timedelta\n",
    "from torch.nn import L1Loss\n",
    "import torch.optim as optim\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import Aurora, Batch, Metadata\n",
    "from aurora import Batch as BaseBatch\n",
    "from aurora import Metadata as BaseMetadata\n",
    "from aurora.batch import interpolate\n",
    "\n",
    "# Import dependencies\n",
    "import contextlib\n",
    "from functools import partial\n",
    "import dataclasses\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from torch.nn import L1Loss  # = MAE: Mean Absolute Error = '.abs().mean()'\n",
    "from torch.nn import Module  # for custom WeightedMAELoss (Aurora loss)\n",
    "import torch.nn.functional as F  # for downsample batch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from peft import get_peft_model\n",
    "import tempfile\n",
    "import requests\n",
    "import pickle\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import gcsfs\n",
    "from typing import Union, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aurora core\n",
    "from aurora import Aurora, Batch, Metadata\n",
    "from aurora.batch import interpolate\n",
    "\n",
    "# Dataset & dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Model training utils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import L1Loss, Module\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast\n",
    "\n",
    "# LoRA PEFT\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# Data processing\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Filesystem + storage\n",
    "import gcsfs\n",
    "import zarr\n",
    "\n",
    "# General utilities\n",
    "from typing import Union, Callable\n",
    "from datetime import datetime, timedelta\n",
    "import dataclasses\n",
    "import tempfile\n",
    "import contextlib\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# Avoid CUDA error: invalid configuration argument on F.scaled_dot_product_attention\n",
    "# https://stackoverflow.com/questions/77343471/pytorch-cuda-error-invalid-configuration-argument\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "\n",
    "# Constants\n",
    "STATIC_VARS_HF_URL = \"https://huggingface.co/microsoft/aurora/resolve/main/aurora-0.25-static.pickle\"\n",
    "GCS_URL = \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721.zarr/\"\n",
    "LEAD_TIME = pd.Timedelta(\"6h\")\n",
    "AURORA_PRESSURE_LEVELS = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]\n",
    "AURORA_VARIABLE_NAMES = {\n",
    "    \"surface\": [\n",
    "        '10u',\n",
    "        '10v',\n",
    "        '2t',\n",
    "        'msl',\n",
    "    ],\n",
    "    \"atmospheric\": [\n",
    "        't',\n",
    "        'u',\n",
    "        'v',\n",
    "        'q',\n",
    "        'z',\n",
    "    ],\n",
    "    \"static\": [\n",
    "        'z',  # geopotential_at_sea_level\n",
    "        'lsm',  # land_sea_mask\n",
    "        'slt',  # soil_type\n",
    "    ]\n",
    "}\n",
    "VARIABLES_STATISTICS: dict[str, tuple[float, float]] = { # mean, std (location, scale)\n",
    "    'z': (-1386.496, 58844.67),\n",
    "    'lsm': (0.0, 1.0),\n",
    "    'slt': (0.0, 7.0),\n",
    "    '2t': (278.514, 21.22036),\n",
    "    '10u': (-0.05135059, 5.547512),\n",
    "    '10v': (0.189158, 4.765339),\n",
    "    'msl': (100957.8, 1332.246),\n",
    "    'z_50': (199373.0, 5875.553),\n",
    "    'z_100': (157642.1, 5510.64),\n",
    "    'z_150': (133141.4, 5823.912),\n",
    "    'z_200': (115330.0, 5820.169),\n",
    "    'z_250': (101223.1, 5536.585),\n",
    "    'z_300': (89414.15, 5091.916),\n",
    "    'z_400': (69980.38, 4150.851),\n",
    "    'z_500': (54115.37, 3353.187),\n",
    "    'z_600': (40648.33, 2695.808),\n",
    "    'z_700': (28928.82, 2136.436),\n",
    "    'z_850': (13749.78, 1470.321),\n",
    "    'z_925': (7015.005, 1228.997),\n",
    "    'z_1000': (738.1545, 1072.307),\n",
    "    'u_50': (5.653076, 15.29281),\n",
    "    'u_100': (10.27951, 13.52611),\n",
    "    'u_150': (13.54061, 16.04335),\n",
    "    'u_200': (14.20915, 17.6763),\n",
    "    'u_250': (13.34584, 17.9671),\n",
    "    'u_300': (11.80173, 17.11917),\n",
    "    'u_400': (8.817291, 14.34276),\n",
    "    'u_500': (6.563273, 11.98419),\n",
    "    'u_600': (4.814521, 10.33421),\n",
    "    'u_700': (3.345237, 9.168821),\n",
    "    'u_850': (1.418379, 8.188043),\n",
    "    'u_925': (0.6172657, 7.940808),\n",
    "    'u_1000': (-0.03328723, 6.141778),\n",
    "    'v_50': (0.004226111, 7.058931),\n",
    "    'v_100': (0.01411897, 7.47931),\n",
    "    'v_150': (-0.03697671, 9.57199),\n",
    "    'v_200': (-0.04507801, 11.88069),\n",
    "    'v_250': (-0.02980338, 13.38039),\n",
    "    'v_300': (-0.0229477, 13.34044),\n",
    "    'v_400': (-0.01771003, 11.22955),\n",
    "    'v_500': (-0.02387986, 9.181708),\n",
    "    'v_600': (-0.02716674, 7.803569),\n",
    "    'v_700': (0.02153583, 6.87104),\n",
    "    'v_850': (0.142815, 6.264443),\n",
    "    'v_925': (0.205348, 6.470644),\n",
    "    'v_1000': (0.1867637, 5.308203),\n",
    "    't_50': (212.4864, 10.26284),\n",
    "    't_100': (208.4042, 12.52901),\n",
    "    't_150': (213.3201, 8.928709),\n",
    "    't_200': (218.0615, 7.189547),\n",
    "    't_250': (222.771, 8.529282),\n",
    "    't_300': (228.8696, 10.71679),\n",
    "    't_400': (242.1368, 12.69102),\n",
    "    't_500': (252.9492, 13.06447),\n",
    "    't_600': (261.1347, 13.42046),\n",
    "    't_700': (267.401, 14.76523),\n",
    "    't_850': (274.56, 15.5888),\n",
    "    't_925': (277.3572, 16.08798),\n",
    "    't_1000': (281.013, 17.13983),\n",
    "    'q_50': (2.67818e-06, 3.571687e-07),\n",
    "    'q_100': (2.633677e-06, 5.703754e-07),\n",
    "    'q_150': (5.254625e-06, 3.794077e-06),\n",
    "    'q_200': (1.940632e-05, 2.267534e-05),\n",
    "    'q_250': (5.773618e-05, 7.446644e-05),\n",
    "    'q_300': (0.0001273861, 0.0001684361),\n",
    "    'q_400': (0.0003855659, 0.0005078644),\n",
    "    'q_500': (0.0008529599, 0.001079294),\n",
    "    'q_600': (0.001541429, 0.001769722),\n",
    "    'q_700': (0.002431637, 0.002549169),\n",
    "    'q_850': (0.004575618, 0.004112368),\n",
    "    'q_925': (0.006033134, 0.005071058),\n",
    "    'q_1000': (0.007030342, 0.005913548)\n",
    "}\n",
    "\n",
    "class Xaurora(Aurora):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        self.autocast = kwargs.pop(\"autocast\", True)  # Remove 'autocast' from kwargs\n",
    "        self.autocast_dtype = torch.bfloat16\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def forward(self, batch: Batch, lead_time: timedelta) -> Batch:\n",
    "        member_id = batch.metadata.member_id\n",
    "\n",
    "        # Move batch to correct dtype and device\n",
    "        p = next(self.parameters())\n",
    "        batch = batch.type(p.dtype)\n",
    "        batch = batch.normalise()\n",
    "        batch = batch.crop(patch_size=self.encoder.patch_size)\n",
    "        batch = batch.to(p.device)\n",
    "\n",
    "        # Determine resolution of latent patch representation\n",
    "        H, W = batch.spatial_shape\n",
    "        patch_res = (\n",
    "            self.encoder.latent_levels,\n",
    "            H // self.encoder.patch_size,\n",
    "            W // self.encoder.patch_size,\n",
    "        )\n",
    "\n",
    "        # Repeat static variables to match batch/time dims\n",
    "        B, T = next(iter(batch.surf_vars.values())).shape[:2]\n",
    "        batch = dataclasses.replace(\n",
    "            batch,\n",
    "            static_vars={k: v[None, None].repeat(B, T, 1, 1) for k, v in batch.static_vars.items()},\n",
    "        )\n",
    "\n",
    "        # Encode inputs\n",
    "        x = self.encoder(batch, lead_time=lead_time)\n",
    "\n",
    "        # Apply backbone with autocast if enabled\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=self.autocast_dtype) if self.autocast else contextlib.nullcontext():\n",
    "            x = self.backbone(\n",
    "                x,\n",
    "                lead_time=lead_time,\n",
    "                patch_res=patch_res,\n",
    "                rollout_step=batch.metadata.rollout_step,\n",
    "            )\n",
    "\n",
    "        # Decode forecast\n",
    "        pred = self.decoder(\n",
    "            x,\n",
    "            batch,\n",
    "            lead_time=lead_time,\n",
    "            patch_res=patch_res,\n",
    "        )\n",
    "\n",
    "        # Wrap in Batch object and restore metadata\n",
    "        pred = Batch.from_aurora_batch(pred, member_id=member_id)\n",
    "        assert pred.metadata.member_id == batch.metadata.member_id, \"Member ID mismatch.\"\n",
    "\n",
    "        # Remove temporal dims from static variables\n",
    "        pred = dataclasses.replace(\n",
    "            pred,\n",
    "            static_vars={k: v[0, 0] for k, v in batch.static_vars.items()},\n",
    "        )\n",
    "\n",
    "        # Ensure output shape includes time dim (1 step)\n",
    "        pred = dataclasses.replace(\n",
    "            pred,\n",
    "            surf_vars={k: v[:, None] for k, v in pred.surf_vars.items()},\n",
    "            atmos_vars={k: v[:, None] for k, v in pred.atmos_vars.items()},\n",
    "            metadata=dataclasses.replace(\n",
    "                pred.metadata,\n",
    "                rollout_step=batch.metadata.rollout_step + 1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Unnormalize the output\n",
    "        pred = pred.unnormalise()\n",
    "        return pred\n",
    "\n",
    "    \n",
    "XauroraSmall = partial(\n",
    "    Xaurora,\n",
    "    encoder_depths=(2, 6, 2),\n",
    "    encoder_num_heads=(4, 8, 16),\n",
    "    decoder_depths=(2, 6, 2),\n",
    "    decoder_num_heads=(16, 8, 4),\n",
    "    embed_dim=256,\n",
    "    num_heads=8,\n",
    "    use_lora=False,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Metadata(BaseMetadata):\n",
    "    \"\"\"Metadata in a batch.\n",
    "\n",
    "    Args:\n",
    "        lat (:class:`torch.Tensor`): Latitudes.\n",
    "        lon (:class:`torch.Tensor`): Longitudes.\n",
    "        time (tuple[datetime, ...]): For every batch element, the time.\n",
    "        atmos_levels (tuple[int | float, ...]): Pressure levels for the atmospheric variables in\n",
    "            hPa.\n",
    "        rollout_step (int, optional): How many roll-out steps were used to produce this prediction.\n",
    "            If equal to `0`, which is the default, then this means that this is not a prediction,\n",
    "            but actual data. This field is automatically populated by the model and used to use a\n",
    "            separate LoRA for every roll-out step. Generally, you are safe to ignore this field.\n",
    "        member_id (int, optional): The member ID of the ensemble member. It defaults to `0`.\n",
    "    \"\"\"\n",
    "\n",
    "    lat: torch.Tensor\n",
    "    lon: torch.Tensor\n",
    "    time: tuple[datetime, ...]\n",
    "    atmos_levels: tuple[int | float, ...]\n",
    "    rollout_step: int = 0\n",
    "    member_id: int|list[int] = None\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Batch:\n",
    "    \"\"\"A batch of data.\n",
    "\n",
    "    Args:\n",
    "        surf_vars (dict[str, :class:`torch.Tensor`]): Surface-level variables with shape\n",
    "            `(b, t, h, w)`.\n",
    "        static_vars (dict[str, :class:`torch.Tensor`]): Static variables with shape `(h, w)`.\n",
    "        atmos_vars (dict[str, :class:`torch.Tensor`]): Atmospheric variables with shape\n",
    "            `(b, t, c, h, w)`.\n",
    "        metadata (:class:`Metadata`): Metadata associated to this batch.\n",
    "    \"\"\"\n",
    "\n",
    "    surf_vars: dict[str, torch.Tensor]\n",
    "    static_vars: dict[str, torch.Tensor]\n",
    "    atmos_vars: dict[str, torch.Tensor]\n",
    "    metadata: Metadata\n",
    "    \n",
    "    @property\n",
    "    def spatial_shape(self) -> tuple[int, int]:\n",
    "        \"\"\"Get the spatial shape from an arbitrary surface-level variable.\"\"\"\n",
    "        return next(iter(self.surf_vars.values())).shape[-2:]\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save the batch to a file.\"\"\"\n",
    "        torch.save(self, path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(path: str) -> \"Batch\":\n",
    "        \"\"\"Load a batch from a file.\"\"\"\n",
    "        return torch.load(path, weights_only=False)\n",
    "    \n",
    "    def fillna(self, how: str=\"spatial_conv\", **how_kwargs):\n",
    "        if how == \"spatial_conv\":\n",
    "            return Batch(\n",
    "                surf_vars={k: fillna_spatial_mean_conv(v, **how_kwargs) for k, v in self.surf_vars.items()},\n",
    "                static_vars={k: fillna_spatial_mean_conv(v, **how_kwargs) for k, v in self.static_vars.items()},\n",
    "                atmos_vars={k: fillna_spatial_mean_conv(v, **how_kwargs) for k, v in self.atmos_vars.items()},\n",
    "                metadata=self.metadata,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fillna method {how}.\")\n",
    "    \n",
    "    def normalise(\n",
    "        self, \n",
    "        stats: dict[str, tuple[float, float]]=VARIABLES_STATISTICS\n",
    "    ) -> \"Batch\":\n",
    "        assert all(\n",
    "            k in stats.keys() for k in self.surf_vars.keys()\n",
    "        ), \"Not all surface variables have statistics.\"\n",
    "        return Batch(\n",
    "            surf_vars={\n",
    "                k: normalise_surf_var(v, k, stats=stats) for k, v in self.surf_vars.items()\n",
    "            },\n",
    "            static_vars={\n",
    "                k: normalise_surf_var(v, k, stats=stats) for k, v in self.static_vars.items()\n",
    "            },\n",
    "            atmos_vars={\n",
    "                k: normalise_atmos_var(v, k, self.metadata.atmos_levels, stats=stats)\n",
    "                for k, v in self.atmos_vars.items()\n",
    "            },\n",
    "            metadata=self.metadata,\n",
    "        )\n",
    "    \n",
    "    def unnormalise(\n",
    "        self, \n",
    "        stats: dict[str, tuple[float, float]]=VARIABLES_STATISTICS\n",
    "    ) -> \"Batch\":\n",
    "        assert all(\n",
    "            k in stats.keys() for k in self.surf_vars.keys()\n",
    "        ), \"Not all surface variables have statistics.\"\n",
    "        return Batch(\n",
    "            surf_vars={\n",
    "                k: unnormalise_surf_var(v, k, stats=stats) for k, v in self.surf_vars.items()\n",
    "            },\n",
    "            static_vars={\n",
    "                k: unnormalise_surf_var(v, k, stats=stats) for k, v in self.static_vars.items()\n",
    "            },\n",
    "            atmos_vars={\n",
    "                k: unnormalise_atmos_var(v, k, self.metadata.atmos_levels, stats=stats)\n",
    "                for k, v in self.atmos_vars.items()\n",
    "            },\n",
    "            metadata=self.metadata,\n",
    "        )\n",
    "        \n",
    "    def crop(self, patch_size: int) -> \"Batch\":\n",
    "        \"\"\"Crop the variables in the batch to patch size `patch_size`.\"\"\"\n",
    "        h, w = self.spatial_shape\n",
    "\n",
    "        if w % patch_size != 0:\n",
    "            raise ValueError(\"Width of the data must be a multiple of the patch size.\")\n",
    "\n",
    "        if h % patch_size == 0:\n",
    "            return self\n",
    "        \n",
    "        elif h % patch_size == 1:\n",
    "            return Batch(\n",
    "                surf_vars={k: v[..., :-1, :] for k, v in self.surf_vars.items()},\n",
    "                static_vars={k: v[..., :-1, :] for k, v in self.static_vars.items()},\n",
    "                atmos_vars={k: v[..., :-1, :] for k, v in self.atmos_vars.items()},\n",
    "                metadata=Metadata(\n",
    "                    lat=self.metadata.lat[:-1],\n",
    "                    lon=self.metadata.lon,\n",
    "                    atmos_levels=self.metadata.atmos_levels,\n",
    "                    time=self.metadata.time,\n",
    "                    rollout_step=self.metadata.rollout_step,\n",
    "                    member_id=self.metadata.member_id,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"There can at most be one latitude too many, \"\n",
    "                f\"but there are {h % patch_size} too many.\"\n",
    "            )\n",
    "            \n",
    "    def crop_right(self, n: int) -> \"Batch\":\n",
    "        \"\"\"Crop the rightmost `n` columns of the variables in the batch.\"\"\"\n",
    "        return Batch(\n",
    "            surf_vars={k: v[..., :-n, :] for k, v in self.surf_vars.items()},\n",
    "            static_vars={k: v[..., :-n, :] for k, v in self.static_vars.items()},\n",
    "            atmos_vars={k: v[..., :-n, :] for k, v in self.atmos_vars.items()},\n",
    "            metadata=Metadata(\n",
    "                lat=self.metadata.lat[:-n],\n",
    "                lon=self.metadata.lon,\n",
    "                atmos_levels=self.metadata.atmos_levels,\n",
    "                time=self.metadata.time,\n",
    "                rollout_step=self.metadata.rollout_step,\n",
    "                member_id=self.metadata.member_id,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _fmap(self, f: Callable[[torch.Tensor], torch.Tensor]) -> \"Batch\":\n",
    "        return Batch(\n",
    "            surf_vars={k: f(v) for k, v in self.surf_vars.items()},\n",
    "            static_vars={k: f(v) for k, v in self.static_vars.items()},\n",
    "            atmos_vars={k: f(v) for k, v in self.atmos_vars.items()},\n",
    "            metadata=Metadata(\n",
    "                lat=f(self.metadata.lat),\n",
    "                lon=f(self.metadata.lon),\n",
    "                atmos_levels=self.metadata.atmos_levels,\n",
    "                time=self.metadata.time,\n",
    "                rollout_step=self.metadata.rollout_step,\n",
    "                member_id=self.metadata.member_id,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def to(self, device: str | torch.device) -> \"Batch\":\n",
    "        \"\"\"Move the batch to another device.\"\"\"\n",
    "        return self._fmap(lambda x: x.to(device))\n",
    "\n",
    "    def type(self, t: type) -> \"Batch\":\n",
    "        \"\"\"Convert everything to type `t`.\"\"\"\n",
    "        return self._fmap(lambda x: x.type(t))\n",
    "\n",
    "    def regrid(self, res: float) -> \"Batch\":\n",
    "        \"\"\"Regrid the batch to a `res` degrees resolution.\n",
    "\n",
    "        This results in `float32` data on the CPU.\n",
    "\n",
    "        This function is not optimised for either speed or accuracy. Use at your own risk.\n",
    "        \"\"\"\n",
    "\n",
    "        shape = (round(180 / res) + 1, round(360 / res))\n",
    "        lat_new = torch.from_numpy(np.linspace(90, -90, shape[0]))\n",
    "        lon_new = torch.from_numpy(np.linspace(0, 360, shape[1], endpoint=False))\n",
    "        interpolate_res = partial(\n",
    "            interpolate,\n",
    "            lat=self.metadata.lat,\n",
    "            lon=self.metadata.lon,\n",
    "            lat_new=lat_new,\n",
    "            lon_new=lon_new,\n",
    "        )\n",
    "\n",
    "        return Batch(\n",
    "            surf_vars={k: interpolate_res(v) for k, v in self.surf_vars.items()},\n",
    "            static_vars={k: interpolate_res(v) for k, v in self.static_vars.items()},\n",
    "            atmos_vars={k: interpolate_res(v) for k, v in self.atmos_vars.items()},\n",
    "            metadata=Metadata(\n",
    "                lat=lat_new,\n",
    "                lon=lon_new,\n",
    "                atmos_levels=self.metadata.atmos_levels,\n",
    "                time=self.metadata.time,\n",
    "                rollout_step=self.metadata.rollout_step,\n",
    "                member_id=self.metadata.member_id,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "    def downsample(self, factor: int) -> \"Batch\":\n",
    "        \"\"\"Downsample batch by factor using Batch.regrid.\n",
    "        \n",
    "        See notes about using it carefully in .regrid\"\"\"\n",
    "        \n",
    "        # detach and to cpu \n",
    "        if self.metadata.lat.device != torch.device('cpu'):\n",
    "            out = self._fmap(lambda x: x.detach().cpu())\n",
    "        else:\n",
    "            out = self\n",
    "        \n",
    "        _, W = out.spatial_shape\n",
    "        current_resolution = 360 / W\n",
    "        new_resolution = current_resolution * factor\n",
    "        \n",
    "        return out.regrid(new_resolution)\n",
    "    \n",
    "    @classmethod        \n",
    "    def from_aurora_batch(cls, aurora_batch: BaseBatch, **metadata_kwargs) -> \"Batch\":\n",
    "        return cls(\n",
    "            surf_vars=aurora_batch.surf_vars,\n",
    "            static_vars=aurora_batch.static_vars,\n",
    "            atmos_vars=aurora_batch.atmos_vars,\n",
    "            metadata=Metadata(\n",
    "                lat=aurora_batch.metadata.lat,\n",
    "                lon=aurora_batch.metadata.lon,\n",
    "                time=aurora_batch.metadata.time,\n",
    "                atmos_levels=aurora_batch.metadata.atmos_levels,\n",
    "                rollout_step=aurora_batch.metadata.rollout_step,\n",
    "                **metadata_kwargs\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class WeightedMAELoss(Module):\n",
    "    \"\"\"\n",
    "    Weighted Mean Absolute Error loss used in Aurora fine-tuning.\n",
    "    Updated to use `reduction='mean'` instead of manually normalizing.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        gamma=1.0,  # dataset weight — reduced from 2.0 to stabilize magnitude\n",
    "        alpha=0.25,  # overall surface loss weight\n",
    "        beta=1.0,  # overall atmospheric loss weight\n",
    "        surf_var_weights=None,  # dict of surface variable weights\n",
    "        atmos_var_weights=None  # dict of atmospheric variable weights\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.l1loss = L1Loss(reduction='mean')  # avg over all pixels/levels\n",
    "\n",
    "        self.surf_var_weights = surf_var_weights or {\n",
    "            \"2t\": 1.0, \"10u\": 1.0, \"10v\": 1.0, \"msl\": 1.0\n",
    "        }\n",
    "        self.atmos_var_weights = atmos_var_weights or {\n",
    "            \"z\": 1.0, \"q\": 1.0, \"t\": 1.0, \"u\": 1.0, \"v\": 1.0\n",
    "        }\n",
    "\n",
    "    def forward(self, pred_batch, target_batch) -> torch.Tensor:\n",
    "        device = next(iter(pred_batch.surf_vars.values())).device\n",
    "\n",
    "        surface_loss_sum = torch.tensor(0.0, device=device)\n",
    "        atmospheric_loss_sum = torch.tensor(0.0, device=device)\n",
    "\n",
    "        for k in pred_batch.surf_vars:\n",
    "            w_S_k = self.surf_var_weights.get(k, 1.0)\n",
    "            var_mae = self.l1loss(pred_batch.surf_vars[k], target_batch.surf_vars[k])\n",
    "            surface_loss_sum += w_S_k * var_mae\n",
    "\n",
    "        for k in pred_batch.atmos_vars:\n",
    "            w_A_k = self.atmos_var_weights.get(k, 1.0)\n",
    "            var_mae = self.l1loss(pred_batch.atmos_vars[k], target_batch.atmos_vars[k])\n",
    "            atmospheric_loss_sum += w_A_k * var_mae\n",
    "\n",
    "        VS = len(pred_batch.surf_vars)\n",
    "        VA = len(pred_batch.atmos_vars)\n",
    "\n",
    "        combined_loss = self.alpha * surface_loss_sum + self.beta * atmospheric_loss_sum\n",
    "        final_loss = (self.gamma / (VS + VA)) * combined_loss\n",
    "\n",
    "        return final_loss\n",
    "\n",
    "\n",
    "\n",
    "class AuroraDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # xr Datasets\n",
    "        surface_ds: xr.Dataset,\n",
    "        atmospheric_ds: xr.Dataset,\n",
    "        static_ds: xr.Dataset,\n",
    "        # variables\n",
    "        surface_variables: list[str]=AURORA_VARIABLE_NAMES[\"surface\"],\n",
    "        atmospheric_variables: list[str]=AURORA_VARIABLE_NAMES[\"atmospheric\"],\n",
    "        static_variables: list[str]=AURORA_VARIABLE_NAMES[\"static\"],\n",
    "        # temporal parameters\n",
    "        base_frequency: Union[str, pd.Timedelta]=\"6h\",\n",
    "        input_temporal_length: Union[str, int, pd.Timedelta]=\"12h\",\n",
    "        output_temporal_length: Union[str, int, pd.Timedelta]=\"6h\",\n",
    "        inter_sample_gap: Union[str, int, pd.Timedelta]=\"6h\",\n",
    "        forecast_horizon: Union[str, int, pd.Timedelta]=\"6w\",\n",
    "        init_frequency: Union[str, int, pd.Timedelta]=\"1d\",\n",
    "        init_gap: Union[str, int, pd.Timedelta]=\"0h\",\n",
    "        downsampling_rate: int=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise the AuroraDataset.\n",
    "\n",
    "        Args:\n",
    "            surface_ds: xr.Dataset\n",
    "                The surface dataset.\n",
    "            atmospheric_ds: xr.Dataset\n",
    "                The atmospheric dataset.\n",
    "            static_ds: xr.Dataset\n",
    "                The static dataset.\n",
    "            surface_variables: list[str]\n",
    "                The surface variables to include. Defaults to Aurora's surface variables.\n",
    "            atmospheric_variables: list[str]\n",
    "                The atmospheric variables to include. Defaults to Aurora's atmospheric variables.\n",
    "            static_variables: list[str]\n",
    "                The static variables to include. Defaults to Aurora's static variables.\n",
    "            base_frequency: Union[str, pd.Timedelta]\n",
    "                The base frequency of the data. Defaults to 6 hours.\n",
    "            input_temporal_length: Union[str, int, pd.Timedelta]\n",
    "                The length of the input time series. If `int`, considered to be a multiple of `base_frequency`. Defaults to 12 hours.\n",
    "            output_temporal_length: Union[str, int, pd.Timedelta]\n",
    "                The length of the output time series. If `int`, considered to be a multiple of `base_frequency`. Defaults to 6 hours.\n",
    "            inter_sample_gap: Union[str, int, pd.Timedelta]\n",
    "                The gap between consecutive samples. If `int`, considered to be a multiple of `base_frequency`. Defaults to 6 hours.\n",
    "            forecast_horizon: Union[str, int, pd.Timedelta]\n",
    "                The forecast horizon. If `int`, considered to be a multiple of `base_frequency`. Defaults to 6 weeks.\n",
    "            init_frequency: Union[str, int, pd.Timedelta]\n",
    "                The frequency of the initialisation times. If `int`, considered to be a multiple of `base_frequency`. Defaults to 1 day.\n",
    "            init_gap: Union[str, int, pd.Timedelta]\n",
    "                The gap between the initialisation time and the start of the input time series. If `int`, considered to be a multiple of \n",
    "                `base_frequency`. Defaults to 0 hours.\n",
    "            downsampling_rate: int\n",
    "                The downsampling rate of the dataset. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # get sorted datasets\n",
    "        # Aurora requires latitudes to be in descending order\n",
    "        # and longitudes to be in ascending order\n",
    "        self.surface_ds = surface_ds.sortby(\"latitude\", ascending=False).sortby(\"longitude\", ascending=True)\n",
    "        self.atmospheric_ds = atmospheric_ds.sortby(\"latitude\", ascending=False).sortby(\"longitude\", ascending=True)\n",
    "        self.static_ds = static_ds.sortby(\"latitude\", ascending=False).sortby(\"longitude\", ascending=True)\n",
    "\n",
    "        self.atmospheric_variables = atmospheric_variables\n",
    "        self.surface_variables = surface_variables\n",
    "        self.static_variables = static_variables\n",
    "\n",
    "        self.base_frequency = pd.Timedelta(base_frequency) if isinstance(base_frequency, str) else base_frequency\n",
    "        self.input_temporal_length = convert_to_steps(input_temporal_length, self.base_frequency, check_valid=True)\n",
    "        self.output_temporal_length = convert_to_steps(output_temporal_length, self.base_frequency, check_valid=True)\n",
    "        self.inter_sample_gap = convert_to_steps(inter_sample_gap, self.base_frequency, check_valid=True)\n",
    "        self.forecast_horizon = convert_to_steps(forecast_horizon, self.base_frequency, check_valid=True)\n",
    "        self.init_frequency = convert_to_steps(init_frequency, self.base_frequency, check_valid=True)\n",
    "        self.init_gap = convert_to_steps(init_gap, self.base_frequency, check_valid=True)\n",
    "\n",
    "        self.downsampling_rate = downsampling_rate\n",
    "        self._spatial_shape = None\n",
    "\n",
    "        # extract timestamps\n",
    "        assert (surface_ds.time == atmospheric_ds.time).all(), f\"got different timestamps for surface and atmospheric data.\"\n",
    "        self.timestamps = self.surface_ds.time.values.astype(\"datetime64[s]\")\n",
    "        \n",
    "        # add init gap\n",
    "        if self.init_gap < 0:\n",
    "            # reverse the init gap index based on the length of self.timestamps\n",
    "            self.init_gap = len(self.timestamps) + self.init_gap\n",
    "        self.timestamps = self.timestamps[self.init_gap:]        \n",
    "\n",
    "        # compute feasible length of the time series\n",
    "        discarded_timesteps = self.forecast_horizon + self.output_temporal_length + self.input_temporal_length + self.inter_sample_gap\n",
    "        self.num_samples = len(self.timestamps) - discarded_timesteps\n",
    "        assert self.timestamps[self.num_samples-1] + discarded_timesteps * self.base_frequency == self.timestamps[-1]\n",
    "\n",
    "        # add init frequency\n",
    "        self.num_samples = self.num_samples // self.init_frequency\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_cloud_storage(\n",
    "        cls,\n",
    "        gcs_url: str,\n",
    "        start_year: str|int=None,\n",
    "        end_year: str|int=None,\n",
    "        static_url: str=STATIC_VARS_HF_URL,\n",
    "        atmospheric_variables: list[str]=AURORA_VARIABLE_NAMES[\"atmospheric\"],\n",
    "        surface_variables: list[str]=AURORA_VARIABLE_NAMES[\"surface\"],\n",
    "        pressure_levels: list[int]=AURORA_PRESSURE_LEVELS,\n",
    "        variable_names_map: dict[str, str]=None,\n",
    "        **kwargs\n",
    "    ) -> \"AuroraDataset\":\n",
    "        \"\"\"\n",
    "        No need to download anything locally!\n",
    "\n",
    "        Args:\n",
    "            gcs_url: str\n",
    "                The url to the zarr store in the cloud storage.\n",
    "            static_url: str\n",
    "                The url to the static dataset in the cloud storage.\n",
    "            atmospheric_variables: list[str]\n",
    "                The atmospheric variables to include. Defaults to Aurora's atmospheric variables.\n",
    "            surface_variables: list[str]\n",
    "                The surface variables to include. Defaults to Aurora's surface variables.\n",
    "            pressure_levels: list[int]\n",
    "                The pressure levels to include. Defaults to Aurora's pressure levels.\n",
    "            variable_names_map: dict[str, str]\n",
    "                A dictionary mapping the variable names to the desired names.\n",
    "            **kwargs:\n",
    "                Additional arguments to pass to the AuroraDataset constructor.\n",
    "        Returns:\n",
    "            AuroraDataset\n",
    "        \"\"\"\n",
    "        static_ds = load_static_ds_local(\"static_data/static.nc\")\n",
    "\n",
    "        # load surface and atmospheric ds\n",
    "        surface_ds, atmospheric_ds = load_gcs_datasets(\n",
    "            gcs_url=gcs_url,\n",
    "            start_year=start_year,\n",
    "            end_year=end_year,\n",
    "            atmospheric_variables=atmospheric_variables,\n",
    "            surface_variables=surface_variables,\n",
    "            pressure_levels=pressure_levels,\n",
    "            variable_names_map=variable_names_map\n",
    "        )\n",
    "        return cls(\n",
    "            surface_ds=surface_ds,\n",
    "            atmospheric_ds=atmospheric_ds,\n",
    "            static_ds=static_ds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def spatial_shape(self) -> tuple[int]:\n",
    "        \"\"\"\n",
    "        return (nlat, nlon)\n",
    "        \"\"\"\n",
    "        # return if already computed\n",
    "        if self._spatial_shape is not None:\n",
    "            return self._spatial_shape\n",
    "        # compute\n",
    "        batch = self[0][0]\n",
    "        if self.downsampling_rate is not None and self.downsampling_rate > 1:\n",
    "            with torch.no_grad():\n",
    "                batch = downsample_batch(batch, self.downsampling_rate)\n",
    "\n",
    "        _,_,H,W = batch.surf_vars[next(iter(batch.surf_vars.keys()))].shape\n",
    "        self._spatial_shape = (H, W)\n",
    "        return self._spatial_shape\n",
    "\n",
    "    def get_matching_timestamp(self, timestamp: np.datetime64|datetime) -> np.datetime64:\n",
    "        \"\"\"\n",
    "        Find a create an OUTPUT batch from the requested timestamp. \n",
    "        \"\"\"\n",
    "        if self.output_temporal_length > 1:\n",
    "            raise NotImplementedError(\"get_matching_timestamp is only implemented for output_temporal_length=1.\")\n",
    "\n",
    "        if isinstance(timestamp, datetime):\n",
    "            timestamp = np.datetime64(timestamp)\n",
    "\n",
    "        # get output slice\n",
    "        output_slice = [\n",
    "            timestamp + np.timedelta64(pd.Timedelta(self.base_frequency).value * n, \"s\")\n",
    "            for n in range(self.output_temporal_length)\n",
    "        ]\n",
    "\n",
    "        # return the batch\n",
    "        return xr_to_batch(\n",
    "            self.surface_ds.sel(time=output_slice).compute(),\n",
    "            self.atmospheric_ds.sel(time=output_slice).compute(),\n",
    "            self.static_ds.compute(),\n",
    "            surface_variables=self.surface_variables,\n",
    "            static_variables=self.static_variables,\n",
    "            atmospheric_variables=self.atmospheric_variables\n",
    "        )\n",
    "\n",
    "    def get_matching_output(self, batch: Batch) -> Batch:\n",
    "        \"\"\"\n",
    "        Creates a Batch from the requested timestamp. The timestamp is the first \n",
    "        timestamp of the output time series of length `output_temporal_length`.\n",
    "        \"\"\"\n",
    "        T = batch.atmos_vars[next(iter(batch.atmos_vars.keys()))].shape[1]\n",
    "        if T != 1:\n",
    "            raise NotImplementedError(\"get_matching_output is only implemented for T=1.\")\n",
    "\n",
    "        # get the times\n",
    "        times = list(batch.metadata.time)\n",
    "        output_batch = []\n",
    "\n",
    "        # loop over requested times\n",
    "        for time in times:\n",
    "            new = self.get_matching_timestamp(time)\n",
    "            output_batch.append(new)\n",
    "\n",
    "        # collate\n",
    "        output_batch = batch_collate_fn(output_batch)\n",
    "\n",
    "        # apply downsampling\n",
    "        if self.downsampling_rate is not None and self.downsampling_rate > 1:\n",
    "            with torch.no_grad():\n",
    "                output_batch = downsample_batch(output_batch, self.downsampling_rate)\n",
    "\n",
    "        return output_batch\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Batch:\n",
    "        if idx < 0:\n",
    "            idx = self.num_samples + idx\n",
    "\n",
    "        # add init frequency\n",
    "        idx = idx * self.init_frequency\n",
    "\n",
    "        # get input and output indexes\n",
    "        input_slice = slice(idx, idx+self.input_temporal_length)\n",
    "        output_slice = slice(idx+self.input_temporal_length+self.inter_sample_gap-1,\n",
    "                             idx+self.input_temporal_length+self.inter_sample_gap-1+self.output_temporal_length)\n",
    "\n",
    "        # get timestamps\n",
    "        input_timestamps = self.timestamps[input_slice]\n",
    "        output_timestamps = self.timestamps[output_slice]\n",
    "\n",
    "        # make a few checks\n",
    "        assert len(input_timestamps) == self.input_temporal_length\n",
    "        assert len(output_timestamps) == self.output_temporal_length\n",
    "        assert input_timestamps[-1] + self.inter_sample_gap * pd.Timedelta(self.base_frequency) == output_timestamps[0]\n",
    "\n",
    "        input_batch = xr_to_batch(\n",
    "            self.surface_ds.sel(time=input_timestamps).compute(),\n",
    "            self.atmospheric_ds.sel(time=input_timestamps).compute(),\n",
    "            self.static_ds.compute(),\n",
    "            surface_variables=self.surface_variables,\n",
    "            static_variables=self.static_variables,\n",
    "            atmospheric_variables=self.atmospheric_variables\n",
    "        )\n",
    "\n",
    "        output_batch = xr_to_batch(\n",
    "            self.surface_ds.sel(time=output_timestamps).compute(),\n",
    "            self.atmospheric_ds.sel(time=output_timestamps).compute(),\n",
    "            self.static_ds.compute(),\n",
    "            surface_variables=self.surface_variables,\n",
    "            static_variables=self.static_variables,\n",
    "            atmospheric_variables=self.atmospheric_variables\n",
    "        )\n",
    "\n",
    "        if self.downsampling_rate is not None and self.downsampling_rate > 1:\n",
    "            with torch.no_grad():\n",
    "                input_batch = downsample_batch(input_batch, self.downsampling_rate)\n",
    "                output_batch = downsample_batch(output_batch, self.downsampling_rate)\n",
    "\n",
    "        return input_batch, output_batch\n",
    "\n",
    "\n",
    "# Functions\n",
    "def normalise_surf_var(\n",
    "    x: torch.Tensor,\n",
    "    name: str,\n",
    "    stats: dict[str, tuple[float, float]],\n",
    "    unnormalise: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Normalise a surface-level variable.\"\"\"\n",
    "    location, scale = stats[name]\n",
    "    if unnormalise:\n",
    "        return x * scale + location\n",
    "    else:\n",
    "        return (x - location) / scale\n",
    "\n",
    "\n",
    "def normalise_atmos_var(\n",
    "    x: torch.Tensor,\n",
    "    name: str,\n",
    "    atmos_levels: tuple[int | float, ...],\n",
    "    stats: dict[str, tuple[float, float]],\n",
    "    unnormalise: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Normalise an atmospheric variable.\"\"\"\n",
    "    level_locations: list[int | float] = []\n",
    "    level_scales: list[int | float] = []\n",
    "    for level in atmos_levels:\n",
    "        name_level = f\"{name}_{level}\"\n",
    "        level_locations.append(stats[name_level][0])\n",
    "        level_scales.append(stats[name_level][1])\n",
    "    location = torch.tensor(level_locations, dtype=x.dtype, device=x.device)\n",
    "    scale = torch.tensor(level_scales, dtype=x.dtype, device=x.device)\n",
    "\n",
    "    if unnormalise:\n",
    "        return x * scale[..., None, None] + location[..., None, None]\n",
    "    else:\n",
    "        return (x - location[..., None, None]) / scale[..., None, None]\n",
    "\n",
    "\n",
    "unnormalise_surf_var = partial(normalise_surf_var, unnormalise=True)\n",
    "unnormalise_atmos_var = partial(normalise_atmos_var, unnormalise=True)\n",
    "\n",
    "\n",
    "def fillna_spatial_mean_conv(tensor: torch.Tensor, kernel_size: int = 3, iterations: int = 5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate filling of NaNs via convolutional local mean.\n",
    "    \n",
    "    Supports:\n",
    "      - 4D (B, T, H, W)\n",
    "      - 5D (B, T, L, H, W)\n",
    "\n",
    "    Args:\n",
    "        tensor: Tensor with NaNs (on GPU or CPU).\n",
    "        kernel_size: Spatial window size (must be odd).\n",
    "        iterations: Number of fill iterations.\n",
    "\n",
    "    Returns:\n",
    "        Tensor with NaNs replaced by local mean.\n",
    "    \"\"\"\n",
    "    assert kernel_size % 2 == 1, \"kernel_size must be odd\"\n",
    "    \n",
    "    if not torch.isnan(tensor).any():\n",
    "        return tensor  # No NaNs — early exit\n",
    "\n",
    "    device = tensor.device\n",
    "    filled = tensor.clone()\n",
    "    nan_mask = torch.isnan(filled)\n",
    "    filled[nan_mask] = 0  # Replace NaNs with 0s for now\n",
    "\n",
    "    # Set up convolution kernel\n",
    "    kernel_dim = 2 if tensor.ndim == 4 else 3\n",
    "    spatial_dims = (-2, -1) if kernel_dim == 2 else (-3, -2, -1)\n",
    "    padding = kernel_size // 2\n",
    "\n",
    "    # Define kernel\n",
    "    kernel_shape = [1] * (tensor.ndim - kernel_dim) + [1] * kernel_dim\n",
    "    kernel = torch.ones(kernel_shape[:-kernel_dim] + [kernel_size] * kernel_dim, device=device)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        valid_mask = ~torch.isnan(tensor)\n",
    "        weights = valid_mask.float()\n",
    "        values = filled * weights\n",
    "\n",
    "        if kernel_dim == 2:\n",
    "            smoothed_vals = F.conv2d(values.view(-1, 1, *values.shape[-2:]), kernel, padding=padding, groups=1)\n",
    "            smoothed_weights = F.conv2d(weights.view(-1, 1, *weights.shape[-2:]), kernel, padding=padding, groups=1)\n",
    "        else:  # 3D\n",
    "            smoothed_vals = F.conv3d(values.view(-1, 1, *values.shape[-3:]), kernel, padding=padding, groups=1)\n",
    "            smoothed_weights = F.conv3d(weights.view(-1, 1, *weights.shape[-3:]), kernel, padding=padding, groups=1)\n",
    "\n",
    "        smoothed_mean = smoothed_vals / (smoothed_weights + 1e-6)\n",
    "        smoothed_mean = smoothed_mean.view_as(filled)\n",
    "\n",
    "        # Update only NaN locations\n",
    "        filled[nan_mask] = smoothed_mean[nan_mask]\n",
    "\n",
    "    return filled\n",
    "\n",
    "\n",
    "def convert_to_steps(temporal_length: Union[str, int, pd.Timedelta], \n",
    "                      base_frequency: Union[str, pd.Timedelta],\n",
    "                      check_valid: bool=True) -> int:\n",
    "    # convert to timedelta\n",
    "    if isinstance(temporal_length, str):\n",
    "        temporal_length = pd.Timedelta(temporal_length)\n",
    "    elif isinstance(temporal_length, int):\n",
    "        return temporal_length\n",
    "    # check if it is a multiple of the base frequency\n",
    "    if check_valid:\n",
    "        assert temporal_length % pd.Timedelta(base_frequency) == pd.Timedelta(0), \\\n",
    "            \"temporal_length must be a multiple of base_frequency\"\n",
    "    return int(temporal_length / base_frequency)\n",
    "\n",
    "\n",
    "def downsample_batch(batch, factor, pooling=F.avg_pool2d):\n",
    "    return batch.downsample(factor)\n",
    "\n",
    "\n",
    "def batch_collate_fn(batches: list[Batch]|None) -> Batch:\n",
    "    \"\"\"\n",
    "    Custom collate function to combine multiple Batch objects. Collating custom\n",
    "    objects such as Batch in PyTorch requires a custom function. For efficient\n",
    "    parallel processing on the GPU, individual samples are combined into a single\n",
    "    larger 'batch'. In the context of Aurora, one 'sample' is one Aurora Batch\n",
    "    instance: all the atmospheric and surface variables for a single point in\n",
    "    time at a specific location.\n",
    "    \"\"\"\n",
    "    # Check whether the input is of type Batch\n",
    "    _batches = batches.copy()\n",
    "    for i, batch in enumerate(_batches):\n",
    "        if batch is None: batches.pop(i)\n",
    "        elif not isinstance(batch, Batch):\n",
    "            raise ValueError(f\"Expected a list of Aurora batches or NoneType, got {type(batch)}\")\n",
    "    batches = _batches\n",
    "\n",
    "    if len(batches) == 0:\n",
    "        return # nothing to batch return None\n",
    "    elif len(batches) == 1:\n",
    "        return batches[0] # nothing to batch return the single batch\n",
    "\n",
    "    # Prediction batches have a single time sample apparently\n",
    "    times = []\n",
    "    for batch in batches:\n",
    "        time = batch.metadata.time\n",
    "        if isinstance(time, (tuple, list)):\n",
    "            times.extend(list(time))\n",
    "        else:\n",
    "            times.append(time)\n",
    "    times = tuple(times)\n",
    "\n",
    "    # batch the data\n",
    "    return Batch(\n",
    "        surf_vars={\n",
    "            var: torch.cat([batch.surf_vars[var] for batch in batches], dim=0)\n",
    "            for var in batches[0].surf_vars\n",
    "        },\n",
    "        atmos_vars={\n",
    "            var: torch.cat([batch.atmos_vars[var] for batch in batches], dim=0)\n",
    "            for var in batches[0].atmos_vars\n",
    "        },\n",
    "        static_vars={\n",
    "            var: batches[0].static_vars[var]\n",
    "            for var in batches[0].static_vars\n",
    "        },\n",
    "        metadata=Metadata(\n",
    "            lat=batches[0].metadata.lat,\n",
    "            lon=batches[0].metadata.lon,\n",
    "            atmos_levels=batches[0].metadata.atmos_levels,\n",
    "            rollout_step=batches[0].metadata.rollout_step,\n",
    "            time=times,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def batch_pairs_collate_fn(batch_pairs: list[tuple[Batch, Batch]]) -> tuple[Batch, Batch]:\n",
    "    \"\"\"\n",
    "    Collate function for batch input/output pairs. This function is a wrapper designed to \n",
    "    handle the common scenario in supervised learning where the DataLoader yields pairs \n",
    "    of (input, target).\n",
    "\n",
    "    Input: It expects a list where each element is a tuple containing an input Batch and a \n",
    "    target Batch. \n",
    "    E.g., [(input_batch_1, target_batch_1), (input_batch_2, target_batch_2), ...] could be \n",
    "    such a list.\n",
    "    \"\"\"\n",
    "    input_batch = batch_collate_fn([batch_pair[0] for batch_pair in batch_pairs])\n",
    "    output_batch = batch_collate_fn([batch_pair[1] for batch_pair in batch_pairs])\n",
    "\n",
    "    return input_batch, output_batch\n",
    "\n",
    "\n",
    "def prepare_lons_lats(lons: np.ndarray, lats: np.ndarray) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Aurora requires decreasing latitudes and increasing longitudes\n",
    "    \"\"\"\n",
    "    if lats[0] < lats[1]: # i.e. increasing\n",
    "        lats = torch.from_numpy(lats[::-1].copy())\n",
    "        flip_lats = True\n",
    "    else:\n",
    "        lats = torch.from_numpy(lats)\n",
    "        flip_lats = False\n",
    "\n",
    "    if lons[0] < lons[1]: # i.e. increasing\n",
    "        lons = torch.from_numpy(lons)\n",
    "        flip_lons = False\n",
    "    else:\n",
    "        lons = torch.from_numpy(lons[::-1].copy())\n",
    "        flip_lons = True\n",
    "\n",
    "    return lons, lats, flip_lats, flip_lons\n",
    "\n",
    "\n",
    "def prepare_array(x: np.ndarray, shape: tuple[int], flip_lons: bool, flip_lats: bool) -> torch.Tensor:\n",
    "    x = x.reshape(shape).copy()\n",
    "    if flip_lons: x = x[...,::-1].copy()\n",
    "    if flip_lats: x = x[...,::-1,:].copy()\n",
    "    return torch.from_numpy(x).clone()\n",
    "\n",
    "\n",
    "def rename_xr_variables(ds: xr.Dataset, variable_names_map: dict[str, str]):\n",
    "    \"\"\"\n",
    "    Directly taken from Eliot's data/utils.py.\n",
    "    \"\"\"\n",
    "    # Intersect with data_vars\n",
    "    variable_names_map = {k: v for k, v in variable_names_map.items() if k in list(ds.data_vars)+list(ds.coords)}\n",
    "\n",
    "    # Rename the variables in the dataset\n",
    "    renamed_ds = ds.rename(variable_names_map)\n",
    "\n",
    "    return renamed_ds\n",
    "\n",
    "\n",
    "def load_gcs_datasets(\n",
    "    gcs_url: str=GCS_URL,\n",
    "    num_debug_timesteps: int=0,\n",
    "    start_year: str|int=None,\n",
    "    end_year: str|int=None,\n",
    "    atmospheric_variables: list[str]=AURORA_VARIABLE_NAMES[\"atmospheric\"],\n",
    "    surface_variables: list[str]=AURORA_VARIABLE_NAMES[\"surface\"],\n",
    "    pressure_levels: list[int]=AURORA_PRESSURE_LEVELS,\n",
    "    variable_names_map: dict[str, str]=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    A modified version of the function in Eliot's data/utils.py, allowing for\n",
    "    selecting only a small number of recent time points for debugging\n",
    "    purposes.\n",
    "    \"\"\"\n",
    "    if start_year is None:\n",
    "        assert end_year is None, \"If start_year is None, end_year must be None\"\n",
    "    if end_year is None:\n",
    "        assert start_year is None, \"If end_year is None, start_year must be None\"\n",
    "\n",
    "    ds = xr.open_dataset(\n",
    "        gcs_url,\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        storage_options={\"token\": \"anon\"},\n",
    "    )\n",
    "\n",
    "    if num_debug_timesteps:  # Debug mode: select a small number of recent time points\n",
    "        ds = ds.isel(time=slice(-num_debug_timesteps, None))\n",
    "\n",
    "        # Create a slice from the selected times\n",
    "        time_slice = slice(ds.time.values[0], ds.time.values[-1])\n",
    "    elif start_year is not None and end_year is not None:\n",
    "        start_date = f\"{start_year}-01-01\"\n",
    "        end_date = f\"{end_year}-12-31\"\n",
    "        time_slice = slice(start_date, end_date)\n",
    "    else:\n",
    "        time_slice = None\n",
    "\n",
    "    # Remove -90 from latitude (i.e. 721 -> 720)\n",
    "    # Solves ValueError: cannot reshape array of size 1036800 into shape (721,1440)\n",
    "    if 90 in ds.latitude.values and -90 in ds.latitude.values:\n",
    "        ds = ds.where(ds.latitude!=-90, drop=True)\n",
    "\n",
    "    # Get the rename map\n",
    "    if variable_names_map is None:\n",
    "        freq = pd.infer_freq(ds.time.values)\n",
    "\n",
    "        if not freq[0].isnumeric():\n",
    "            freq = \"1\" + freq\n",
    "\n",
    "        if pd.Timedelta(freq) in [pd.Timedelta(\"6h\"), pd.Timedelta(\"24h\")]:\n",
    "            variable_names_map = ERA5_HRES_T0_WB2_VARIABLE_NAMES_MAP\n",
    "        else:\n",
    "            raise ValueError(f\"No variable name map for freq {freq}\")\n",
    "\n",
    "    if len(surface_variables) > 0:\n",
    "        surface_ds = rename_xr_variables(\n",
    "            ds,\n",
    "            variable_names_map\n",
    "        )[surface_variables]\n",
    "\n",
    "        if time_slice is not None:\n",
    "            surface_ds = surface_ds.sel(time=time_slice)\n",
    "            surface_ds = surface_ds.sortby(\"latitude\", ascending=False).sortby(\"longitude\", ascending=True)\n",
    "    else:\n",
    "        surface_ds = None\n",
    "\n",
    "    if len(atmospheric_variables) > 0:\n",
    "        atmospheric_ds = rename_xr_variables(\n",
    "            ds,\n",
    "            variable_names_map\n",
    "        )[atmospheric_variables].sel(level=pressure_levels)\n",
    "\n",
    "        if time_slice is not None:\n",
    "            atmospheric_ds = atmospheric_ds.sel(time=time_slice)\n",
    "            atmospheric_ds = atmospheric_ds.sortby(\"latitude\", ascending=False).sortby(\"longitude\", ascending=True)\n",
    "    else:\n",
    "        atmospheric_ds = None\n",
    "\n",
    "    return surface_ds, atmospheric_ds\n",
    "\n",
    "\n",
    "def xr_to_batch(\n",
    "    surface_ds: xr.Dataset,\n",
    "    atmospheric_ds: xr.Dataset,\n",
    "    static_ds: xr.Dataset,\n",
    "    surface_variables: list[str]=AURORA_VARIABLE_NAMES[\"surface\"],\n",
    "    static_variables: list[str]=AURORA_VARIABLE_NAMES[\"static\"],\n",
    "    atmospheric_variables: list[str]=AURORA_VARIABLE_NAMES[\"atmospheric\"],\n",
    ") -> Batch:\n",
    "    \"\"\"\n",
    "    Create an Aurora Batch from XR Datasets.\n",
    "\n",
    "    inspired by https://microsoft.github.io/aurora/example_era5.html\n",
    "    and https://microsoft.github.io/aurora/example_hres_t0.html\n",
    "    \"\"\"\n",
    "    # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "    # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "    # one value for every batch element.\n",
    "    # temporally, we want index 0 to be PREVIOUS time step and index 1 to be CURRENT time step\n",
    "    # the metadata 'time' refers to the CURRENT time step, i.e. the last index\n",
    "    if surface_ds.sizes[\"time\"] == 1:\n",
    "        _time = (surface_ds.time.values[0].astype(\"datetime64[s]\").item(), )\n",
    "    else:\n",
    "        times = list(sorted(surface_ds.time.values.astype(\"datetime64[s]\").tolist()))\n",
    "        _time = (times[-1],) # only the last\n",
    "\n",
    "    # get shapes for explicit reshaping and get lons, lats, levels\n",
    "    # the process is repeated for each dataset because the\n",
    "    # datasets can be empty\n",
    "    if static_ds is not None and len(static_ds) > 0:\n",
    "        H, W = static_ds.sizes[\"latitude\"], static_ds.sizes[\"longitude\"]\n",
    "\n",
    "        # 1. get lons, lats\n",
    "        lons = static_ds.longitude.values.copy()\n",
    "        lats = static_ds.latitude.values.copy()\n",
    "\n",
    "        # 2. prepare lons, lats\n",
    "        lons, lats, flip_lats_static, flip_lons_static = prepare_lons_lats(lons, lats)\n",
    "\n",
    "    else:\n",
    "        flip_lats_static = False\n",
    "        flip_lons_static = False\n",
    "\n",
    "    if surface_ds is not None and len(surface_ds) > 0:\n",
    "        # sort time\n",
    "        surface_ds = surface_ds.sortby(\"time\", ascending=True)\n",
    "        T, H, W = surface_ds.sizes[\"time\"], surface_ds.sizes[\"latitude\"], surface_ds.sizes[\"longitude\"]\n",
    "\n",
    "        # 1. get lons, lats\n",
    "        lons = surface_ds.longitude.values.copy()\n",
    "        lats = surface_ds.latitude.values.copy()\n",
    "\n",
    "        # 2. prepare lons, lats\n",
    "        lons, lats, flip_lats_surface, flip_lons_surface = prepare_lons_lats(lons, lats)\n",
    "    else:\n",
    "        flip_lats_surface = False\n",
    "        flip_lons_surface = False\n",
    "\n",
    "    if atmospheric_ds is not None and len(atmospheric_ds) > 0:\n",
    "        # sort time\n",
    "        atmospheric_ds = atmospheric_ds.sortby(\"time\", ascending=True)\n",
    "        C, T, H, W = (\n",
    "            atmospheric_ds.sizes[\"level\"],\n",
    "            atmospheric_ds.sizes[\"time\"],\n",
    "            atmospheric_ds.sizes[\"latitude\"],\n",
    "            atmospheric_ds.sizes[\"longitude\"],\n",
    "        )\n",
    "        # 1. get lons, lats\n",
    "        lons = atmospheric_ds.longitude.values.copy()\n",
    "        lats = atmospheric_ds.latitude.values.copy()\n",
    "\n",
    "        # 2. prepare lons, lats\n",
    "        lons, lats, flip_lats_atmospheric, flip_lons_atmospheric = prepare_lons_lats(lons, lats)\n",
    "\n",
    "        # 3. get levels\n",
    "        levels = tuple(int(level) for level in atmospheric_ds.level.values)\n",
    "    else:\n",
    "        levels = tuple()\n",
    "        flip_lats_atmospheric = False\n",
    "        flip_lons_atmospheric = False\n",
    "\n",
    "    return Batch.from_aurora_batch(\n",
    "        BaseBatch(\n",
    "            surf_vars = {\n",
    "                var: prepare_array(\n",
    "                    surface_ds[var].values,\n",
    "                    (1, T, H, W),\n",
    "                    flip_lats=flip_lats_surface,\n",
    "                    flip_lons=flip_lons_surface)\n",
    "                for var in surface_variables\n",
    "            },\n",
    "            atmos_vars = {\n",
    "                var: prepare_array(\n",
    "                    atmospheric_ds[var].values,\n",
    "                    (1, T, C, H, W),\n",
    "                    flip_lats=flip_lats_atmospheric,\n",
    "                    flip_lons=flip_lons_atmospheric)\n",
    "                for var in atmospheric_variables\n",
    "            },\n",
    "            static_vars = {\n",
    "                var: prepare_array(\n",
    "                    static_ds[var].values,\n",
    "                    (H, W),\n",
    "                    flip_lats=flip_lats_static,\n",
    "                    flip_lons=flip_lons_static)\n",
    "                for var in static_variables\n",
    "            } if static_ds is not None and len(static_ds) > 0 else {},\n",
    "            metadata=BaseMetadata(\n",
    "                lat=lats,\n",
    "                lon=lons,\n",
    "                time=_time,\n",
    "                atmos_levels=levels,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def load_static_ds_local(path: str) -> xr.Dataset:\n",
    "    ds = xr.open_dataset(path, engine=\"netcdf4\")\n",
    "    if 90 in ds.latitude.values and -90 in ds.latitude.values:\n",
    "        ds = ds.where(ds.latitude!=-90, drop=True)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Modified version of 'ERA5_HRES_T0_WB2_VARIABLE_NAMES_MAP_6HR' in Eliot's data/constants.py\n",
    "ERA5_HRES_T0_WB2_VARIABLE_NAMES_MAP = {\n",
    "    # Surface-level Variables\n",
    "    '10m_u_component_of_wind': '10u',\n",
    "    '10m_v_component_of_wind': '10v',\n",
    "    '2m_temperature': '2t',\n",
    "    'mean_sea_level_pressure': 'msl',\n",
    "\n",
    "    # Atmospheric Variables\n",
    "    'temperature': 't',\n",
    "    'u_component_of_wind': 'u',\n",
    "    'v_component_of_wind': 'v',\n",
    "    'specific_humidity': 'q',\n",
    "    'geopotential': 'z',\n",
    "}\n",
    "\n",
    "# Initialize the base (pre-trained) Aurora model\n",
    "base_model = XauroraSmall(use_lora=False, autocast=True)\n",
    "base_model.load_checkpoint(\n",
    "    \"microsoft/aurora\", \n",
    "    \"aurora-0.25-small-pretrained.ckpt\", \n",
    "    strict=False,  # to avoid error when loading state dict after disabling lora (finetuned model has lora weights)\n",
    ")\n",
    "\n",
    "names_target_modules = [\n",
    "    \"backbone.encoder_layers.0.blocks.0.attn.qkv\",\n",
    "    \"backbone.encoder_layers.0.blocks.0.attn.proj\",\n",
    "    \"backbone.encoder_layers.0.blocks.1.attn.qkv\",\n",
    "    \"backbone.encoder_layers.0.blocks.1.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.0.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.0.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.1.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.1.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.2.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.2.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.3.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.3.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.4.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.4.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.5.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.5.attn.proj\",\n",
    "    \"backbone.encoder_layers.2.blocks.0.attn.qkv\",\n",
    "    \"backbone.encoder_layers.2.blocks.0.attn.proj\",\n",
    "    \"backbone.encoder_layers.2.blocks.1.attn.qkv\",\n",
    "    \"backbone.encoder_layers.2.blocks.1.attn.proj\",\n",
    "    \"backbone.decoder_layers.0.blocks.0.attn.qkv\",\n",
    "    \"backbone.decoder_layers.0.blocks.0.attn.proj\",\n",
    "    \"backbone.decoder_layers.0.blocks.1.attn.qkv\",\n",
    "    \"backbone.decoder_layers.0.blocks.1.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.0.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.0.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.1.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.1.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.2.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.2.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.3.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.3.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.4.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.4.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.5.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.5.attn.proj\",\n",
    "    \"backbone.decoder_layers.2.blocks.0.attn.qkv\",\n",
    "    \"backbone.decoder_layers.2.blocks.0.attn.proj\",\n",
    "    \"backbone.decoder_layers.2.blocks.1.attn.qkv\",\n",
    "    \"backbone.decoder_layers.2.blocks.1.attn.proj\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dataset = AuroraDataset.from_cloud_storage(\n",
    "    gcs_url=\"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721.zarr/\",  \n",
    "    start_year=2016,\n",
    "    end_year=2020\n",
    ")\n",
    "\n",
    "# Extract datasets for regional slicing\n",
    "surface_ds = dataset.surface_ds\n",
    "atmospheric_ds = dataset.atmospheric_ds\n",
    "static_ds = dataset.static_ds\n",
    "#For fine tuning do fro 2016-2020 including 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import Aurora\n",
    "from peft import LoraConfig, LoraModel\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Region Config ---\n",
    "REGIONS = {\n",
    "    \"eastern_med\": {\n",
    "        \"lat\": slice(43.75, 30.0),\n",
    "        \"lon\": slice(16.0, 43.75)\n",
    "    },\n",
    "    \"western_med\": {\n",
    "        \"lat\": slice(45.75, 32.0),\n",
    "        \"lon\": slice(348.0, 376.0)\n",
    "    }\n",
    "}\n",
    "\n",
    "def build_model():\n",
    "    target_modules = [\n",
    "        f\"backbone.encoder_layers.{i}.blocks.0.attn.qkv\" for i in range(6)\n",
    "    ] + [\n",
    "        f\"backbone.encoder_layers.{i}.blocks.0.attn.proj\" for i in range(6)\n",
    "    ] + [\n",
    "        f\"backbone.decoder_layers.{i}.blocks.0.attn.qkv\" for i in range(6)\n",
    "    ] + [\n",
    "        f\"backbone.decoder_layers.{i}.blocks.0.attn.proj\" for i in range(6)\n",
    "    ]\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    base = Xaurora(use_lora=True, autocast=True)\n",
    "    base.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-pretrained.ckpt\", strict=False)\n",
    "    model = LoraModel(base, config, adapter_name=\"default\")\n",
    "\n",
    "    # Patch forward\n",
    "    original_forward = model.forward\n",
    "    def patched_forward(self, batch, lead_time):\n",
    "        p = next(self.parameters())\n",
    "        batch = batch.type(p.dtype).to(p.device)\n",
    "        return original_forward(batch, lead_time)\n",
    "    model.forward = patched_forward.__get__(model, type(model))\n",
    "\n",
    "    model = model.cuda().train()\n",
    "    model.configure_activation_checkpointing()\n",
    "\n",
    "    # Print trainable params\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_batch_to_device(batch, device):\n",
    "    return batch.to(device)\n",
    "\n",
    "def clean_batch(batch):\n",
    "    for var_group in [batch.surf_vars, batch.atmos_vars, batch.static_vars]:\n",
    "        for k, v in var_group.items():\n",
    "            var_group[k] = v.nan_to_num(nan=0.0).float()\n",
    "    return batch\n",
    "\n",
    "def roll_forward(input_batch, prediction):\n",
    "    new_batch = dataclasses.replace(input_batch)\n",
    "\n",
    "    for group in ['surf_vars', 'atmos_vars']:\n",
    "        input_vars = getattr(input_batch, group)\n",
    "        pred_vars = getattr(prediction, group)\n",
    "        new_group = {}\n",
    "\n",
    "        for k in input_vars:\n",
    "\n",
    "            rolled = torch.cat([input_vars[k][:, -1:], pred_vars[k][:, -1:].detach()], dim=1)\n",
    "            new_group[k] = rolled\n",
    "\n",
    "        setattr(new_batch, group, new_group)\n",
    "\n",
    "    return new_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_region_14d(region_key, surface_ds, atmospheric_ds, static_ds, epochs=5, max_steps=200):\n",
    "    print(f\"\\nStarting fine-tuning for region: {region_key}\")\n",
    "\n",
    "    region = REGIONS[region_key]\n",
    "    surf_reg = surface_ds.sel(latitude=region[\"lat\"], longitude=region[\"lon\"])\n",
    "    atmos_reg = atmospheric_ds.sel(latitude=region[\"lat\"], longitude=region[\"lon\"])\n",
    "    static_reg = static_ds.sel(latitude=region[\"lat\"], longitude=region[\"lon\"])\n",
    "\n",
    "    dataset = AuroraDataset(\n",
    "        surface_ds=surf_reg,\n",
    "        atmospheric_ds=atmos_reg,\n",
    "        static_ds=static_reg,\n",
    "        input_temporal_length=1,\n",
    "        output_temporal_length=\"6h\",\n",
    "        forecast_horizon=\"14d\"\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=lambda x: x[0])\n",
    "    model = build_model()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "    loss_fn = WeightedMAELoss(\n",
    "        gamma=2.0, alpha=0.25, beta=1.0,\n",
    "        surf_var_weights={\"2t\": 3.0, \"10u\": 1.0, \"10v\": 1.0, \"msl\": 0.05},\n",
    "        atmos_var_weights={\"t\": 3.0, \"u\": 1.0, \"v\": 1.0, \"q\": 0.1, \"z\": 0.05},\n",
    "    )\n",
    "\n",
    "    LEAD_TIME = timedelta(hours=6)\n",
    "    ROLLOUT_STEPS = 56\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        for step, (input_batch, _) in enumerate(loader):\n",
    "            if step >= max_steps:\n",
    "                print(\"Max steps reached. Breaking.\")\n",
    "                break\n",
    "\n",
    "            device = next(model.parameters()).device\n",
    "            input_batch = clean_batch(move_batch_to_device(input_batch, device))\n",
    "\n",
    "            # Get start time\n",
    "            initial_time = input_batch.metadata.time\n",
    "            if isinstance(initial_time, (list, tuple, np.ndarray)):\n",
    "                initial_time = initial_time[0]\n",
    "            if isinstance(initial_time, torch.Tensor):\n",
    "                initial_time = initial_time.item()\n",
    "            if isinstance(initial_time, np.datetime64):\n",
    "                initial_time = pd.to_datetime(str(initial_time)).to_pydatetime()\n",
    "\n",
    "            # Build target rollout\n",
    "            target_batches = []\n",
    "            for i in range(ROLLOUT_STEPS):\n",
    "                target_time = initial_time + i * LEAD_TIME\n",
    "                target = dataset.get_matching_timestamp(target_time)\n",
    "                target = clean_batch(move_batch_to_device(target, device)).normalise()\n",
    "                target_batches.append(target)\n",
    "\n",
    "            current_input = input_batch\n",
    "            step_losses = []\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for t in range(ROLLOUT_STEPS):\n",
    "                target = target_batches[t]\n",
    "                with autocast(device_type=\"cuda\"):\n",
    "                    pred = model(current_input, LEAD_TIME)\n",
    "                    pred = clean_batch(pred).normalise()\n",
    "                    for var in pred.surf_vars:\n",
    "                        pred.surf_vars[var] = torch.clamp(pred.surf_vars[var], -1.0, 1.0)\n",
    "                    for var in pred.atmos_vars:\n",
    "                        pred.atmos_vars[var] = torch.clamp(pred.atmos_vars[var], -1.0, 1.0)\n",
    "                    loss = loss_fn(pred, target)\n",
    "\n",
    "                if not torch.isfinite(loss):\n",
    "                    print(f\"[Warning] Non-finite loss at step {t}, setting to zero.\")\n",
    "                    loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "                step_losses.append(loss)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    detached_pred = Batch(\n",
    "                        surf_vars={k: v.detach() for k, v in pred.surf_vars.items()},\n",
    "                        static_vars={k: v.detach() for k, v in pred.static_vars.items()},\n",
    "                        atmos_vars={k: v.detach() for k, v in pred.atmos_vars.items()},\n",
    "                        metadata=pred.metadata\n",
    "                    )\n",
    "                    current_input = roll_forward(current_input, detached_pred)\n",
    "\n",
    "                del pred, target, loss\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            total_loss = sum(step_losses) / len(step_losses)\n",
    "            scaler.scale(total_loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            print(f\"Step {step+1} | Mean Loss: {total_loss.item():.4f}\")\n",
    "            del step_losses, total_loss, current_input\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    ckpt_name = f\"aurora_{region_key}_14d_finetuned_epochss{epochs}.pt\"\n",
    "    torch.save(model.state_dict(), ckpt_name)\n",
    "    print(f\"Model saved to {ckpt_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fine-tuning for region: western_med\n",
      "trainable params: 3,194,880 / 1,259,495,056 (0.25%)\n",
      "\n",
      "Epoch 1/1\n",
      "Step 1 | Mean Loss: 1.3965\n",
      "Step 2 | Mean Loss: 1.3753\n",
      "Step 3 | Mean Loss: 1.5722\n",
      "Step 4 | Mean Loss: 1.6296\n",
      "Step 5 | Mean Loss: 1.7905\n",
      "Step 6 | Mean Loss: 1.4943\n",
      "Step 7 | Mean Loss: 1.5974\n",
      "Step 8 | Mean Loss: 1.3030\n",
      "Step 9 | Mean Loss: 1.7901\n",
      "Step 10 | Mean Loss: 1.4778\n",
      "Max steps reached. Breaking.\n",
      "Model saved to aurora_western_med_14d_finetuned_epochss1.pt\n"
     ]
    }
   ],
   "source": [
    "train_on_region_14d(\"western_med\", surface_ds, atmospheric_ds, static_ds, epochs=1, max_steps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aurora-envi)",
   "language": "python",
   "name": "aurora-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
